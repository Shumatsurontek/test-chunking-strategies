TEXT = """
üß† GUIDE COMPLET : Tool Calling + Streaming avec LangChain et Bedrock

## üîç Contexte

Dans LangChain, l'utilisation combin√©e de **streaming token par token** et de **tool calling structur√©** fonctionne parfaitement avec OpenAI (`ChatOpenAI`), mais pr√©sente de nombreuses limitations avec les mod√®les h√©berg√©s sur Amazon Bedrock (`ChatBedrock`, `ChatBedrockConverse`), comme Claude, Mistral ou Llama.

Tu utilises `ChatBedrock` avec `streaming=True` et constates que :
- Le tool calling ne fonctionne pas en streaming.
- `with_structured_output()` n‚Äôest pas pris en charge.
- Tu dois parser manuellement du JSON dans un flux de texte brut.

---

## üöß Limitations de ChatBedrock

1. ‚ùå **Pas de tool calling structur√© en streaming**
   - Le mod√®le stream du texte brut, sans m√©tadonn√©es structur√©es comme `.tool_call_chunks`.

2. ‚ùå **`with_structured_output()` incompatible avec streaming**
   - N√©cessite `streaming=False`, ce qui d√©sactive le stream token par token.

3. ‚ùå **Interception manuelle n√©cessaire**
   - Tu dois bufferiser le texte et d√©tecter manuellement les tool calls (en JSON).

---

## ‚úÖ Solutions disponibles

### ‚úÖ 1. Streaming manuel + JSON parsing
Tu peux :
- Streamer token par token.
- Assembler les tokens dans un buffer.
- Parser en JSON d√®s que possible (`json.loads()`).
- Ex√©cuter le tool Python associ√©.
- Optionnellement interrompre le flux.

### ‚úÖ 2. Non-streaming + `with_structured_output()`
Tu d√©sactives le stream (`streaming=False`) et obtiens un `TypedDict` ou Pydantic en retour :

```python
from langchain_aws import ChatBedrock
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel

class ToolCall(BaseModel):
    name: str
    arguments: dict

llm = ChatBedrock(model_id="...", streaming=False)
structured_llm = llm.with_structured_output(ToolCall)
result = structured_llm.invoke("Appelle la m√©t√©o pour Paris")
"""

